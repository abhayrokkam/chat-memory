{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_file_path = '../data/memories.txt'\n",
    "\n",
    "with open(memory_file_path, 'r') as file:\n",
    "    memories = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_memory = \"(family) User has a daughter named Margot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(memory: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a memory to the list of memories.\n",
    "\n",
    "    This function appends a given memory (string) to the `memories` list.\n",
    "\n",
    "    Args:\n",
    "        memory (str): The memory to be saved. It should be a string containing the memory.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It modifies the `memories` list in place.\n",
    "    \"\"\"\n",
    "    memories.append(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_memory(target_memory: str, replacement_memory: str) -> None:\n",
    "    \"\"\"\n",
    "    Replace a memory in the memory list with a new one.\n",
    "\n",
    "    This function searches for a specific memory in the `memories` list and replaces it with a new memory. \n",
    "\n",
    "    Args:\n",
    "        target_memory (str): The memory to be replaced. It should be a string that\n",
    "                              matches an existing memory in the list.\n",
    "        replacement_memory (str): The new memory that will replace the target memory.\n",
    "                                  This should be a string containing the replacement memory.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It modifies the `memories` list in place.\n",
    "    \"\"\"\n",
    "    for i, memory in enumerate(memories):\n",
    "        if memory == target_memory:\n",
    "            memories[i] = replacement_memory\n",
    "            \n",
    "            print(f\"[INFO] Memory has been replaced\")\n",
    "            print(f\"[INFO] Old Memory: {target_memory}  |  New Memory: {replacement_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class save_memory(BaseModel):\n",
    "    \"\"\"\n",
    "    Save a memory to the list of memories.\n",
    "    \"\"\"\n",
    "    memory: str = Field(..., description=\"The memory to be saved.\")\n",
    "\n",
    "class replace_memory(BaseModel):\n",
    "    \"\"\"\n",
    "    Replace a memory in the memory list with a new one.\n",
    "    \"\"\"\n",
    "    target_memory: str = Field(..., description=\"The old memory to be replaced.\")\n",
    "    replacement_memory: str = Field(..., description=\"The new memory that will replace the target memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_format = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "You have to give reasoning using the incoming memory.\n",
    "The solution should include the given format after the reasoning: {function_format}.\n",
    "The solution can be NONE.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "You have access to the following functions:\n",
    "\n",
    "{convert_pydantic_to_openai_function(save_memory)}\n",
    "{convert_pydantic_to_openai_function(replace_memory)}\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond with NONE.<|im_end|>\n",
    "\n",
    "<|im_start|>User Memories\n",
    "{memories}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>Incoming Memory\n",
    "{incoming_memory}\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>question\n",
    "User Memories show the saved memories in the system.\n",
    "If the Incoming Memory or a similar memory already exists, your solution should be NONE.\n",
    "If the Incoming Memory or a similar memory does not exist in the User Memories, save the memory.\n",
    "If the Incoming Memory or a similar memory contradicts a memory in the User Memories, replace the old memory with a new memory.\n",
    "\n",
    "Which function should I use for the Incoming Memory?\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>Solution\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                          torch_dtype=torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.update(\n",
    "    **{\n",
    "        **{\n",
    "            \"use_cache\": True,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 0,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_tokens = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(generated_tokens.squeeze(), skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chat-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
